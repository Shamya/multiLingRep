{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "from libraries.tweetUtilities import tweetTokenizer, tweetPreProcess\n",
    "from libraries.sentiment_classifier import AvgVector, vector, classifyusingAvgVectors, w2vec_model, classify, sentimentAnalysisSpanishDataset\n",
    "def sentimentAnalysisEnglishDataset():\n",
    "    FILE = 'data/EnglishSentimentDataSet/downloaded_tweets.tsv'\n",
    "    X = []\n",
    "    Y = []\n",
    "    with open(FILE, 'rb') as csvfile:\n",
    "        spamreader = csv.reader(csvfile, delimiter='\\t')\n",
    "        for row in spamreader:\n",
    "            if( len(row[3:]) > 1):\n",
    "                Tweet = ' '.join(row[3:])\n",
    "            else:\n",
    "                if(row[3]=='Not Available'):\n",
    "                    continue\n",
    "                Tweet = row[3]\n",
    "            if(row[2]=='positive'):\n",
    "                Y.append(1)\n",
    "            elif(row[2]=='neutral'):\n",
    "                Y.append(0)\n",
    "            elif(row[2]=='negative'):\n",
    "                Y.append(-1)\n",
    "            else:\n",
    "                assert False\n",
    "            X.append(tweetPreProcess(Tweet,'english'));\n",
    "    return np.array(X), Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X,Y = sentimentAnalysisEnglishDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indices_pos = [i for i, e in enumerate(Y) if e == 1]\n",
    "indices_neg = [i for i, e in enumerate(Y) if e == -1]\n",
    "indices_neu = [i for i, e in enumerate(Y) if e == 0]\n",
    "neu_len = len(indices_neu)\n",
    "neg_len = len(indices_neg)\n",
    "pos_len = len(indices_pos)\n",
    "\n",
    "\n",
    "train_X_neu  =  X[indices_neu][:neu_len/2]\n",
    "train_X_neg  =  X[indices_neg][:neg_len/2]\n",
    "train_X_pos  =  X[indices_pos][:pos_len/2]\n",
    "\n",
    "train_Y_neu  = np.full((len(train_X_neu)), 0)\n",
    "train_Y_neg  = np.full((len(train_X_neg)), 0)\n",
    "train_Y_pos  = np.full((len(train_X_pos)), 1)\n",
    "\n",
    "test_X_neu  =  X[indices_neu][neu_len/2:]\n",
    "test_X_neg  =  X[indices_neg][neg_len/2:]\n",
    "test_X_pos  =  X[indices_pos][pos_len/2:]\n",
    "\n",
    "\n",
    "test_Y_neu  = np.full((len(test_X_neu)), 0)\n",
    "test_Y_neg  = np.full((len(test_X_neg)), 0)\n",
    "test_Y_pos  = np.full((len(test_X_pos)), 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_X = np.concatenate((train_X_neg,train_X_pos), axis=0)\n",
    "train_Y = np.concatenate((train_Y_neg,train_Y_pos), axis=0)\n",
    "test_X = np.concatenate((test_X_neg,test_X_pos), axis=0)\n",
    "test_Y = np.concatenate((test_Y_neg,test_Y_pos), axis=0)\n",
    "entire_english_X = np.concatenate((train_X,test_X), axis=0)\n",
    "entire_english_Y = np.concatenate((train_Y,test_Y), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_english_df = pd.DataFrame({'Sentence':(train_X),'ClassifiedOutput':(train_Y)})\n",
    "test_english_df =  pd.DataFrame({'Sentence':(test_X),'ClassifiedOutput':(test_Y)})\n",
    "entire_english_df = pd.DataFrame({'Sentence':(entire_english_X),'ClassifiedOutput':(entire_english_Y)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING WORD2VEC MODEL - esp\n",
      "LOADED WORD2VEC MODEL - esp\n",
      "[1 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 1 1\n",
      " 1 0 1 0 1 1 0 1 1 0 0 0 1 0 1 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1\n",
      " 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 0 0 1 0 1 1 1 0 0 1 1 1 1 0 0 1 1 1\n",
      " 1 0 1 0 1 0 0 0 1 1 1 1 1 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 0 1 0 0 1 0 1 1\n",
      " 1 1 0 1 0 0 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 0 0 1 0 0 1 0 1 0 1 1 0 1 1\n",
      " 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0\n",
      " 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 0 0 1 1 1 0 0 0 0 0 0 1 1 0 1 1 1 1 0 1 1\n",
      " 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1\n",
      " 0 1 1 0 0 1 1 1 1 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 0 0 1\n",
      " 1 0 0 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 0 0 1\n",
      " 0 1 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 0 0 1 0 1 1 0 1 1 1 0\n",
      " 1 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 0 1 1 1\n",
      " 1 1 1 1 1 1 0 0 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0\n",
      " 1 1 0 1 1 1 0 1 1 1 0 0 0 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 1 0 1 0 0 0 1 1 0 1 1 0 0 1 0 1\n",
      " 1 0 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 0 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1\n",
      " 1 0 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
      " 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 1 1 1 1 1 0\n",
      " 1 1 1 1 0 0 1 1 1 0 0 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 0 0 0 1 1\n",
      " 0 1 1]\n",
      "{'tn': 209.0, 'fp': 0.0, 'fn': 0.0, 'tp': 570.0}\n",
      "accuracy = 1.0\n",
      "F score = 1.0\n",
      "{'tn': 75.0, 'fp': 135.0, 'fn': 151.0, 'tp': 419.0}\n",
      "accuracy = 0.633333333333\n",
      "F score = 0.745551601423\n"
     ]
    }
   ],
   "source": [
    "%autoreload 2\n",
    "classifyusingAvgVectors(train_english_df,test_english_df,dimensionOfVector=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/SpanishSentimentDataSet/Train/general-tweets-train-tagged.xml\n",
      "5066\n",
      "data/SpanishSentimentDataSet/Train/politics2013-tweets-test-tagged.xml\n",
      "1337\n",
      "6403\n"
     ]
    }
   ],
   "source": [
    "train_spanish_df, test_spanish_df, entire_spanish_df = sentimentAnalysisSpanishDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING WORD2VEC MODEL - mul\n",
      "LOADED WORD2VEC MODEL - mul\n",
      "[1 1 1 0 1 0 1 1 1 0 1 1 1 1 1 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 1 1 0 1 1\n",
      " 1 0 0 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0\n",
      " 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1 0 0 1 0 0 1 1 0\n",
      " 1 0 0 0 1 0 0 1 1 0 1 0 1 0 0 1 0 0 1 0 1 0 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0\n",
      " 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0 1 1\n",
      " 0 1 0 1 0 0 1 1 0 1 1 1 0 1 0 0 0 1 1 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 1\n",
      " 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 0 0\n",
      " 1 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 0 0 1 1 1 0 1 1 1 0 1\n",
      " 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 0 0 1 0 1 1 1 0 0 0 1\n",
      " 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0\n",
      " 0 1 1 1 0 1 1 1 0 1 0 0 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0\n",
      " 1 0 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 0 0 0 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1\n",
      " 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 0 1\n",
      " 1 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1\n",
      " 1 1 1]\n",
      "{'tn': 209.0, 'fp': 0.0, 'fn': 0.0, 'tp': 570.0}\n",
      "Precision :  1.0\n",
      "Recall :  1.0\n",
      "F score :  1.0\n",
      "Accuracy :  1.0\n",
      "{'tn': 107.0, 'fp': 103.0, 'fn': 99.0, 'tp': 471.0}\n",
      "Precision :  0.820557491289\n",
      "Recall :  0.820557491289\n",
      "F score :  0.823426573427\n",
      "Accuracy :  0.741025641026\n"
     ]
    }
   ],
   "source": [
    "%autoreload 2\n",
    "classifyusingAvgVectors(train_english_df,test_english_df,dimensionOfVector=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING WORD2VEC MODEL - mul\n",
      "LOADED WORD2VEC MODEL - mul\n",
      "[1 0 1 ..., 0 0 1]\n",
      "{'tn': 1606.0, 'fp': 576.0, 'fn': 515.0, 'tp': 2369.0}\n",
      "Precision :  0.80441426146\n",
      "Recall :  0.821428571429\n",
      "F score :  0.812832389775\n",
      "Accuracy :  0.784642716147\n",
      "{'tn': 425.0, 'fp': 273.0, 'fn': 277.0, 'tp': 362.0}\n",
      "Precision :  0.570078740157\n",
      "Recall :  0.566510172144\n",
      "F score :  0.568288854003\n",
      "Accuracy :  0.588631264024\n"
     ]
    }
   ],
   "source": [
    "classifyusingAvgVectors(train_spanish_df,test_spanish_df,dimensionOfVector=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING WORD2VEC MODEL - mul\n",
      "LOADED WORD2VEC MODEL - mul\n",
      "LOADING WORD2VEC MODEL - esp\n",
      "LOADED WORD2VEC MODEL - esp\n",
      "LOADING WORD2VEC MODEL - eng\n",
      "LOADED WORD2VEC MODEL - eng\n"
     ]
    }
   ],
   "source": [
    "mul_model = w2vec_model('mul')\n",
    "span_model = w2vec_model('es')\n",
    "eng_model = w2vec_model('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "w2v_en_model = gensim.models.KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "es_en_train_df = pd.concat([train_english_df, train_spanish_df], ignore_index=True)\n",
    "es_en_test_df = pd.concat([test_english_df, test_spanish_df], ignore_index=True)\n",
    "es_en_entire_df = pd.concat([entire_english_df, entire_spanish_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Train :  779\n",
      "Length of Test :  780\n",
      "[1 0 1 0 0 0 0 1 0 0 1 0 0 1 1 0 1 1 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 1 1 0 1\n",
      " 1 0 0 0 1 1 0 0 1 1 0 1 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 0 1 1 1 0 0\n",
      " 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0\n",
      " 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 1 0 0 1 1 0 1 0 1 0 0 1\n",
      " 0 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 0\n",
      " 0 1 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1\n",
      " 0 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1\n",
      " 0 1 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 1\n",
      " 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 0 0 1 1 1 1 0 0\n",
      " 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1\n",
      " 1 0 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1\n",
      " 0 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 0\n",
      " 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1\n",
      " 0 1 0]\n",
      "{'tn': 209.0, 'fp': 0.0, 'fn': 0.0, 'tp': 570.0}\n",
      "Precision :  1.0\n",
      "Recall :  1.0\n",
      "F score :  1.0\n",
      "Accuracy :  1.0\n",
      "{'tn': 130.0, 'fp': 80.0, 'fn': 84.0, 'tp': 486.0}\n",
      "Precision :  0.858657243816\n",
      "Recall :  0.852631578947\n",
      "F score :  0.855633802817\n",
      "Accuracy :  0.789743589744\n"
     ]
    }
   ],
   "source": [
    "train_df = train_english_df\n",
    "test_df = test_english_df\n",
    "print \"Length of Train : \", str(len(train_df))\n",
    "print \"Length of Test : \", str(len(test_df))\n",
    "classifyusingAvgVectors(train_df,test_df,dimensionOfVector=300, model=w2v_en_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy\n",
      "{'tn': 209.0, 'fp': 0.0, 'fn': 0.0, 'tp': 570.0}\n",
      "Precision :  1.0\n",
      "Recall :  1.0\n",
      "F score :  1.0\n",
      "Accuracy :  1.0\n",
      "Test Accuracy\n",
      "{'tn': 57.0, 'fp': 153.0, 'fn': 44.0, 'tp': 526.0}\n",
      "Precision :  0.774668630339\n",
      "Recall :  0.922807017544\n",
      "F score :  0.842273819055\n",
      "Accuracy :  0.747435897436\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(     ClassifiedOutput                                           Sentence\n",
       " 0                   0  Kapan sih lo ngebuktiin jan ngomong doang Susa...\n",
       " 1                   0  Activists Deir Ezzor captured image Musab Bin ...\n",
       " 2                   0  For International Women day let forget trans w...\n",
       " 3                   0  I condemned measures relating religion cheap I...\n",
       " 4                   0  haha well get lost I gotta back Oklahoma Monda...\n",
       " 5                   0  I got drank I mix lean I throw money I walk gr...\n",
       " 6                   0                                 fuck SATs tomorrow\n",
       " 7                   0              oo nga eh may picture sana ako PREPIX\n",
       " 8                   0  NJEA Teacher Convention Nov 8th amp 9th Atlant...\n",
       " 9                   0  Trey Burke suspended men basketball team exhib...\n",
       " 10                  0  If grandmother sister may Sorry I sounded like...\n",
       " 11                  0  MT Deir Ezzor Mohammed Al Saleh martyred due s...\n",
       " 12                  0  I get work Bucks game Thurs Im gonna fake high...\n",
       " 13                  0  Halloween wore next list Danksgiving Nov 23 Bi...\n",
       " 14                  0  Someone must happy hired help Sewickley Sidney...\n",
       " 15                  0  September 22 Bastard Bearded Irishmen half way...\n",
       " 16                  0  Renewed fighting rocks Syria An early morning ...\n",
       " 17                  0  DTN Cricket Pattinson may saved A tour ahead A...\n",
       " 18                  0  Every day work Monday Paley screening Tuesday ...\n",
       " 19                  0  well shit apparently supposed wake Kony 2012 p...\n",
       " 20                  0                 im going Fair tomorrow Fuck hahaha\n",
       " 21                  0  How Dylan Quirk make Vic 19 He kept James Patt...\n",
       " 22                  0  My Mrs arrested 4 streaking Crufts The Judge g...\n",
       " 23                  0  Louisville Peyton Siva quality Big East Provid...\n",
       " 24                  0  Turn Apple snarky statement Samsung quite judg...\n",
       " 25                  0  When lying bed phone desk Siri pops I dead tom...\n",
       " 26                  0                   I think I going JOUVERT tomorrow\n",
       " 27                  0           going reason I fail HSPA tomorrow I know\n",
       " 28                  0  I blow soon amp even remember day last june ju...\n",
       " 29                  0  Lack LTE may cause Google Nexus 4 flop While G...\n",
       " ..                ...                                                ...\n",
       " 749                 1            All Eden Hazard 5th goal great ball 5 3\n",
       " 750                 1  Apple Special Event OMG iMac paper thin gorgeo...\n",
       " 751                 1                         able come cuba march break\n",
       " 752                 1  As mom said I bitter THSK always hold honor pr...\n",
       " 753                 1  It Great Big Beautiful Tomorrow Do like outfit...\n",
       " 754                 1  back 09 followed Club Paradise ratchet 6th row...\n",
       " 755                 1  Despite going Saturday looks like Ian Bennett ...\n",
       " 756                 1  Lolol pulled house blasting Great Big Beautifu...\n",
       " 757                 1   All four Chevys running top five Lap 239 4th YAY\n",
       " 758                 1  Love given Triple S encouraging try best every...\n",
       " 759                 1  Want help reelect President Join us tomorrow 7...\n",
       " 760                 1  Oh thursday Listening pandora 1st gt Taking Ba...\n",
       " 761                 1  Don hit Twitter bookings features Contact rans...\n",
       " 762                 1  hey Marni going kings next sunday would love a...\n",
       " 763                 1  Moving tomorrow Go Tesco near And takeaways ba...\n",
       " 764                 1  An opportunity serve Gazette Halloween Parade ...\n",
       " 765                 1  rmb something xD called jonghyun 1st come show...\n",
       " 766                 1            This may greatest thing I done life ME3\n",
       " 767                 1  Boris enjoyed view perched top bushels apples ...\n",
       " 768                 1               Tomorrow KARWA CHOTH Delhi going MAD\n",
       " 769                 1  Yes us UFO Sighting Surprise Arizona October 3...\n",
       " 770                 1  Aja treats friends tomorrow The card says Aja ...\n",
       " 771                 1  Colts defensive coordinator Greg Mausky confir...\n",
       " 772                 1  best documentary feature goes Saving Face haha...\n",
       " 773                 1  would Raps kick tires Jordan Farmar Would soli...\n",
       " 774                 1  Consistently Thursday perfect midway point 6 m...\n",
       " 775                 1  Comedy Show w Speaks TOMORROW Houston Improv 7...\n",
       " 776                 1  Best In Show Paris Fashion Week Roundup As BeT...\n",
       " 777                 1  Siri vs Google Voice Search may best robot hel...\n",
       " 778                 1  come visit Gregory Gym Saturday Orange vs Whit...\n",
       " \n",
       " [779 rows x 2 columns],\n",
       "      ClassifiedOutput                                           Sentence\n",
       " 0                   0  We lost HQ Wills Kate Olympics till mid Septem...\n",
       " 1                   0  8th Mar 191 Cao Cao attempted assassination Do...\n",
       " 2                   0  ESPN Chris Mortensen reported fear SLB Scott F...\n",
       " 3                   0  Bears suffer 2 1 loss SDSU tonight next No 4 5...\n",
       " 4                   0  What Petraeus tell Intel Comm Sept 13 blame video\n",
       " 5                   0         Win Bowyer And 3rd time drive Victory Lane\n",
       " 6                   0  got science PSSA tomorrow point retaking study...\n",
       " 7                   0  Okay change plans startin Sydney dalton diet t...\n",
       " 8                   0  Can sleep Got lot mind If I get WGC tourney Ru...\n",
       " 9                   0  Nearly 14 minute mark 3rd quarter Michael Dyer...\n",
       " 10                  0       Michael Dyer may dumbest young man alive smh\n",
       " 11                  0  Nobody knows DRose game ever return March seem...\n",
       " 12                  0  u cant tweet sum1 GUESS WHO IMMA SEE THURSDAY ...\n",
       " 13                  0  Gators get caught looking ahead March So tell ...\n",
       " 14                  0  This BNP never get elected They may put suits ...\n",
       " 15                  0  Report Michael Dyer stopped March gun marijuan...\n",
       " 16                  0  Thur Kick Tebow new girlfriend Mario Williams ...\n",
       " 17                  0  I made game game predictions I think take brea...\n",
       " 18                  0  nolove I put Do take upload 2nd time example Y...\n",
       " 19                  0  I always thought Michael Dyer amp Isiah Crowel...\n",
       " 20                  0  The VP Senegalese Football Federation resigned...\n",
       " 21                  0  Book Depository may cheap inferior printing qu...\n",
       " 22                  0  Don wait til last minute book booths get tix S...\n",
       " 23                  0  Mother Day coming like tomorrow today idk prep...\n",
       " 24                  0  Billy Gillispie awful Nothing worse coach Unle...\n",
       " 25                  0  I know greater indictment current GOP violatin...\n",
       " 26                  0  MSA Poo Poo kind day If experiencing trouble s...\n",
       " 27                  0         went HMV meek mills album come till Monday\n",
       " 28                  0            Just remembered I taking PSSAs tomorrow\n",
       " 29                  0  Coming Weekend Argus tomorrow Kalk Bay waves c...\n",
       " ..                ...                                                ...\n",
       " 750                 1  Omg I want run fingers thru Michael Easton gor...\n",
       " 751                 1  I think let Rushers pick date I suggest Novemb...\n",
       " 752                 1  Fly Madness William Hill 5 4 real price 4 6 Hu...\n",
       " 753                 1  Come support Center Art Performance UCLA next ...\n",
       " 754                 1  They really creative things going Tapanco Call...\n",
       " 755                 1  Amazing things learn Shaker restaurant Saturda...\n",
       " 756                 1  1D aww Thank U lt 13 Have Nice Swift Day good ...\n",
       " 757                 1  indy Reggie Wayne got rocked goalline Sunday a...\n",
       " 758                 1  Da bulls good drose gets back I figure b solid...\n",
       " 759                 1  Its I love West Leeds festival going Pudsey Pa...\n",
       " 760                 1  NASCAR Jimmie Johnson wins 7th time Dover John...\n",
       " 761                 1  I need everybody come video shoot tomorrow sup...\n",
       " 762                 1  Okay I watch How To Rock I may look Max Shneid...\n",
       " 763                 1  since Pittsburgh shows love give back place st...\n",
       " 764                 1  Omar Shaban says Gaza Strip may poor West Bank...\n",
       " 765                 1  With SNSD new single time gig 7th Heaven think...\n",
       " 766                 1  We delighted bene asked play Slackers Whelans ...\n",
       " 767                 1       I get gotta get Andy Lee shirt Thursday game\n",
       " 768                 1  Sweeney Todd incredible Shame finishes Saturda...\n",
       " 769                 1  Nikki Beach Miami Staff ready Welcome MADE IN ...\n",
       " 770                 1        May odds ever favor Happy Halloween Capitol\n",
       " 771                 1          I agree wit 2nd east long DRose sits year\n",
       " 772                 1  Hopefully going genera physics side 6th years ...\n",
       " 773                 1  Good morning Sunday n Mother Day way Wat Su Pa...\n",
       " 774                 1  Well theme UAN WINTER SEASON I feel HOT Hotter...\n",
       " 775                 1  Haha I meant think I taking KL Live assignment...\n",
       " 776                 1  U may well Slam Tent day till Roses Whole day ...\n",
       " 777                 1  S ones taking OGT tomorrow stay focused amp ge...\n",
       " 778                 1  Good afternoon Ian wishing good day fun filled...\n",
       " 779                 1  Don hide desk It salsa bomb dropping Metroplex...\n",
       " \n",
       " [780 rows x 2 columns])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify(train_english_df, test_english_df)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
