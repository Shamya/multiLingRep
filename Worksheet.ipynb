{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spanish Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from xml.dom.minidom import parse as pr\n",
    "tree = pr('general-tweets-train-tagged.xml')\n",
    "tweetsTree = tree.documentElement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets = tweetsTree.getElementsByTagName(\"tweet\")\n",
    "data = []\n",
    "for tweet in tweets:\n",
    "    try:\n",
    "        content = tweet.getElementsByTagName(\"content\")[0].childNodes[0].data\n",
    "    except:\n",
    "        continue\n",
    "    polarity = tweet.getElementsByTagName(\"sentiments\")[0].getElementsByTagName(\"value\")[0].childNodes[0].data\n",
    "    data.append([content, polarity])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7218\n"
     ]
    }
   ],
   "source": [
    "print len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generic Functions (run first)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get accuracy and F score\n",
    "def print_accuracy_zscore(Y_pred, Y_test):\n",
    "    count = 0\n",
    "    correct = 0\n",
    "    scores = {'tp':0,'tn':0,'fp':0,'fn':0}\n",
    "\n",
    "    for i in range(len(Y_pred)):\n",
    "        count += 1\n",
    "        if Y_pred[i] == Y_test[i]:\n",
    "            correct += 1 \n",
    "\n",
    "        #precision and recall\n",
    "        #true positive \n",
    "        if Y_test[i] == 1:\n",
    "            if Y_pred[i] == 1:\n",
    "                scores['tp'] += 1\n",
    "            else:\n",
    "                scores['fn'] += 1\n",
    "        else:\n",
    "            if Y_pred[i] == 1:\n",
    "                scores['fp'] += 1\n",
    "            else:\n",
    "                scores['tn'] += 1\n",
    "    \n",
    "    print scores\n",
    "    test_acc = correct / count \n",
    "    precision = scores['tp'] / (scores['tp'] + scores['fp'])\n",
    "    recall = scores['tp'] / (scores['tp'] + scores['fn'])\n",
    "    z_score = (2*precision*recall)/(precision+recall)\n",
    "    print \"accuracy =\", test_acc\n",
    "    print \"Z score =\", z_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model 1 - bow en-en**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "from itertools import chain, count\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# Global class labels.\n",
    "POS_LABEL = 'pos'\n",
    "NEG_LABEL = 'neg'\n",
    "label_to_dict = {POS_LABEL:1, NEG_LABEL:0}\n",
    "\n",
    "# Path to dataset\n",
    "PATH_TO_DATA = \"/Users/shamya/Documents/multiLingRep/aclImdb\"\n",
    "TRAIN_DIR = os.path.join(PATH_TO_DATA, \"train\")\n",
    "TEST_DIR = os.path.join(PATH_TO_DATA, \"test\")\n",
    "\n",
    "def word_to_index_util():\n",
    "    \"\"\"\n",
    "    Convert word to index \n",
    "    \"\"\"\n",
    "    vocab = list(line.strip() for line in open(os.path.join(PATH_TO_DATA, \"imdb.vocab\")))\n",
    "    index_to_word = list(set(vocab))\n",
    "    word_to_index = dict(zip(index_to_word, count()))\n",
    "    return word_to_index\n",
    "\n",
    "def tokenize_doc(doc):\n",
    "    \"\"\"\n",
    "\n",
    "    Tokenize a document and return its bag-of-words representation.\n",
    "    doc - a string representing a document.\n",
    "    returns a dictionary mapping each word to the number of times it appears in doc.\n",
    "    \"\"\"\n",
    "    bow = defaultdict(float)\n",
    "    #tokens = doc.split()\n",
    "    #lowered_tokens = map(lambda t: t.lower(), tokens)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(doc)\n",
    "    tokens = map(lambda t: t.lower(), tokens)\n",
    "    #remove stop words\n",
    "    #filtered_words = filter(lambda token: token not in stopwords.words('english'), tokens)\n",
    "    for token in tokens:\n",
    "        if token.isalpha(): \n",
    "            bow[token] += 1.0\n",
    "    return bow\n",
    "\n",
    "def string_to_vector(bow, word_to_index):\n",
    "    \"\"\"\n",
    "    Convert the review string into a feature vector (count of words)\n",
    "    \"\"\"\n",
    "    feat_vec = [0] * len(word_to_index)\n",
    "    for word in bow:\n",
    "        try:\n",
    "            feat_vec[word_to_index[word]] = bow[word]\n",
    "        except:\n",
    "            continue\n",
    "    return feat_vec\n",
    "        \n",
    "def process_data_np_array(dir_path):\n",
    "    \"\"\"\n",
    "    Converts input data into an np array for sklearn use\n",
    "    \"\"\"\n",
    "    word_to_index = word_to_index_util()\n",
    "    data = np.zeros((1,len(word_to_index)+1))\n",
    "    pos_path = os.path.join(dir_path, POS_LABEL)\n",
    "    neg_path = os.path.join(dir_path, NEG_LABEL)\n",
    "    print \"Starting training with paths %s and %s\" % (pos_path, neg_path)\n",
    "    for (p, label) in [ (pos_path, POS_LABEL), (neg_path, NEG_LABEL) ]:\n",
    "        filenames = os.listdir(p)\n",
    "        count = 0 \n",
    "        for f in filenames:\n",
    "            with open(os.path.join(p,f),'r') as doc:\n",
    "                count += 1\n",
    "                content = doc.read()\n",
    "                feat_vec = string_to_vector(tokenize_doc(content), word_to_index)\n",
    "                feat_vec = [label_to_dict[label]] + feat_vec\n",
    "                data = np.append(data, [feat_vec], 0)\n",
    "                if count > 700:\n",
    "                    break\n",
    "                \n",
    "    data = np.delete(data, 0, 0) #clear 1st junk row\n",
    "    return data\n",
    "\n",
    "train_data = process_data_np_array(TRAIN_DIR)\n",
    "test_data = process_data_np_array(TEST_DIR)\n",
    "print \"%positive cases in training\", (np.sum(train_data[:,0])/train_data.shape[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test correctness of feature vectors\n",
    "def word_to_index_util():\n",
    "    \"\"\"\n",
    "    Convert word to index \n",
    "    \"\"\"\n",
    "    vocab = list(line.strip() for line in open(os.path.join(PATH_TO_DATA, \"imdb.vocab\")))\n",
    "    index_to_word = list(set(vocab))\n",
    "    word_to_index = dict(zip(index_to_word, count()))\n",
    "    return word_to_index\n",
    "word_to_index = word_to_index_util()\n",
    "print test_data[0,word_to_index[\"ashton\"]+1] #enter test word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save data\n",
    "np.save('train.npy',train_data)\n",
    "np.save('test.npy',train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get in sklearn variables\n",
    "#train_data = np.load('train.npy')\n",
    "#test_data = np.load('test.npy')\n",
    "np.random.shuffle(train_data)\n",
    "np.random.shuffle(test_data)\n",
    "X_train = train_data[:,1:]\n",
    "Y_train = train_data[:,0]\n",
    "X_test = test_data[:,1:]\n",
    "Y_test = test_data[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#logistic regression\n",
    "from sklearn import linear_model \n",
    "lr = linear_model.LogisticRegression()\n",
    "lr.fit(X_train, Y_train)\n",
    "Y_pred = lr.predict(X_test)\n",
    "print_accuracy_zscore(Y_pred, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model 2 - Embedding word2vec -> en-en**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#word2vec\n",
    "import gensim\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "#Download https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit\n",
    "def w2vec_model():\n",
    "  print \"LOADING WORD2VEC MODEL\"\n",
    "  model = gensim.models.Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "  print \"LOADED WORD2VEC MODEL\"\n",
    "  return model\n",
    "w2v_model = w2vec_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from sklearn import svm\n",
    "#from sklearn.linear_models import LinearRegression as lr\n",
    "import numpy as np\n",
    "import os\n",
    "#from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Path to dataset\n",
    "PATH_TO_DATA = \"/Users/shamya/Documents/multiLingRep/aclImdb\"\n",
    "TRAIN_DIR = os.path.join(PATH_TO_DATA, \"train\")\n",
    "TEST_DIR = os.path.join(PATH_TO_DATA, \"test\")\n",
    "POS_LABEL = 'pos'\n",
    "NEG_LABEL = 'neg'\n",
    "\n",
    "\n",
    "def tokensOfDocument(doc):\n",
    "    #return word_tokenize(document)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(doc)\n",
    "    tokens = map(lambda t: t.lower(), tokens)\n",
    "    #remove stop words\n",
    "    #filtered_words = filter(lambda token: token not in stopwords.words('english'), tokens)\n",
    "    return tokens\n",
    "\n",
    "def convertDocumentToVector(w2v_model, document, dimensionOfVector):\n",
    "    tokens = tokensOfDocument(document)\n",
    "    Vector = np.zeros((dimensionOfVector,))\n",
    "    for token in tokens:\n",
    "        if(token in w2v_model):\n",
    "            vec_temp = w2v_model[token]\n",
    "            Vector += vec_temp - np.mean(vec_temp)\n",
    "    return Vector/len(tokens)\n",
    "\n",
    "def processInputSet(inputDoc, w2v_model, inputAnswers, dimensionOfVector):\n",
    "    inputY = inputAnswers\n",
    "    inputX = convertDocumentToVector(w2v_model, inputDoc, dimensionOfVector)\n",
    "    return inputX, inputY\n",
    "\n",
    "def process_data_np_array(dir_path):\n",
    "    \"\"\"\n",
    "    Converts input data into an np array for sklearn use\n",
    "    \"\"\"\n",
    "    data = np.zeros((1,300))\n",
    "    output = []\n",
    "    pos_path = os.path.join(dir_path, POS_LABEL)\n",
    "    neg_path = os.path.join(dir_path, NEG_LABEL)\n",
    "    print \"Starting training with paths %s and %s\" % (pos_path, neg_path)\n",
    "    for (p, label) in [ (pos_path, 1), (neg_path, 0) ]:\n",
    "        filenames = os.listdir(p)\n",
    "        count = 0 \n",
    "        for f in filenames:\n",
    "            with open(os.path.join(p,f),'r') as doc:\n",
    "                count += 1\n",
    "                content = doc.read()\n",
    "                X, Y = processInputSet(content, w2v_model, label, 300)\n",
    "                data = np.append(data, [X], 0)\n",
    "                output.append(Y)\n",
    "                if count > 700:\n",
    "                    break\n",
    "                \n",
    "    data = np.delete(data, 0, 0) #clear 1st junk row\n",
    "    print data.shape\n",
    "    return data, output\n",
    "\n",
    "\n",
    "trainX, trainY = process_data_np_array(TRAIN_DIR)\n",
    "testX, testY = process_data_np_array(TEST_DIR)\n",
    "\n",
    "clf = svm.SVC() #lr\n",
    "clf.fit(trainX, trainY) \n",
    "y_pred = clf.predict(testX)\n",
    "print len(testY) - np.sum(np.abs(testY - y_pred))/len(testY)\n",
    "print_accuracy_zscore(y_pred, testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model 3 - Embedding self-trained -> en-en **"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
